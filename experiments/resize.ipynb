{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T07:32:49.894902Z",
     "start_time": "2024-04-25T07:32:49.887392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Param\n",
    "TOTAL_TIMESTEP_NUMB = 800_000\n",
    "LEARNING_RATE = 0.0001\n",
    "GAE = 1.0\n",
    "ENT_COEF = 0.01\n",
    "N_STEPS = 512\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Test Param\n",
    "EVAL_FREQ = 10000\n",
    "TEST_EPISODE_NUMBERS = 20"
   ],
   "id": "190e8d813e519441",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T07:32:49.910412Z",
     "start_time": "2024-04-25T07:32:49.896406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path('./resize')"
   ],
   "id": "356c5b9f625e9698",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T07:32:49.925924Z",
     "start_time": "2024-04-25T07:32:49.911912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model_cnn import MarioNet\n",
    "\n",
    "policy_kwargs_84 = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")\n",
    "policy_kwargs_42 = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "policy_kwargs_21 = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    ")"
   ],
   "id": "5e5076bbc86f9dfc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T07:33:04.856549Z",
     "start_time": "2024-04-25T07:32:49.926924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import make_parallel_env, STAGE_RECTANGLE\n",
    "\n",
    "env_84 = make_parallel_env(STAGE_RECTANGLE, 8)\n",
    "env_42 = make_parallel_env(STAGE_RECTANGLE, 8, resize=42)\n",
    "env_21 = make_parallel_env(STAGE_RECTANGLE, 8, resize=21)\n",
    "\n",
    "# dict with the different environments and names\n",
    "models = {\n",
    "    'resize_42': dict(env=env_42, policy_kwargs=policy_kwargs_42, eval=make_parallel_env(STAGE_RECTANGLE, 1, resize=42)),\n",
    "    'resize_21': dict(env=env_21, policy_kwargs=policy_kwargs_21, eval=make_parallel_env(STAGE_RECTANGLE, 1, resize=21))\n",
    "}"
   ],
   "id": "ae5282df420a1cc",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T09:30:02.327001Z",
     "start_time": "2024-04-25T07:33:04.859550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import TrainAndLoggingCallback\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "for key in models.keys():\n",
    "    print(f\"Training {key}\")\n",
    "    model = PPO('CnnPolicy', models[key]['env'], verbose=0, policy_kwargs=models[key]['policy_kwargs'], tensorboard_log=save_dir,\n",
    "                learning_rate=LEARNING_RATE, n_steps=N_STEPS, batch_size=BATCH_SIZE, n_epochs=N_EPOCHS, gamma=GAMMA, gae_lambda=GAE, ent_coef=ENT_COEF)\n",
    "    callback = TrainAndLoggingCallback(test_env=models[key]['eval'], check_freq=EVAL_FREQ, episode_num=TEST_EPISODE_NUMBERS)\n",
    "    \n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, tb_log_name=key, callback=callback)\n",
    "    \n",
    "    "
   ],
   "id": "b979069e0aa609ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resize_42\n",
      "time steps: 80000\n",
      "average reward: 745.0 average time: 185.55 best_reward: 1886.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\PycharmProjects\\bsc_thesis\\utils.py:166: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  Video(th.ByteTensor([screens]), fps=40),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps: 160000\n",
      "average reward: 895.65 average time: 197.1 best_reward: 1653.0\n",
      "time steps: 240000\n",
      "average reward: 1141.7 average time: 226.6 best_reward: 1678.0\n",
      "time steps: 320000\n",
      "average reward: 1267.45 average time: 238.15 best_reward: 1834.0\n",
      "time steps: 400000\n",
      "average reward: 1715.75 average time: 325.3 best_reward: 2341.0\n",
      "time steps: 480000\n",
      "average reward: 1559.6 average time: 290.9 best_reward: 2343.0\n",
      "time steps: 560000\n",
      "average reward: 2118.0 average time: 405.3 best_reward: 3025.0\n",
      "time steps: 640000\n",
      "average reward: 2058.25 average time: 385.15 best_reward: 3027.0\n",
      "time steps: 720000\n",
      "average reward: 2162.95 average time: 424.0 best_reward: 3029.0\n",
      "time steps: 800000\n",
      "average reward: 2166.3 average time: 407.1 best_reward: 3027.0\n",
      "Training resize_21\n",
      "time steps: 80000\n",
      "average reward: 750.7 average time: 172.4 best_reward: 1886.0\n",
      "time steps: 160000\n",
      "average reward: 694.65 average time: 162.65 best_reward: 1307.0\n",
      "time steps: 240000\n",
      "average reward: 778.35 average time: 180.95 best_reward: 1532.0\n",
      "time steps: 320000\n",
      "average reward: 981.9 average time: 216.25 best_reward: 1316.0\n",
      "time steps: 400000\n",
      "average reward: 985.25 average time: 218.55 best_reward: 2328.0\n",
      "time steps: 480000\n",
      "average reward: 927.55 average time: 193.45 best_reward: 1540.0\n",
      "time steps: 560000\n",
      "average reward: 1000.45 average time: 214.3 best_reward: 1544.0\n",
      "time steps: 640000\n",
      "average reward: 1117.6 average time: 226.9 best_reward: 1679.0\n",
      "time steps: 720000\n",
      "average reward: 1242.75 average time: 244.9 best_reward: 2327.0\n",
      "time steps: 800000\n",
      "average reward: 1130.85 average time: 217.95 best_reward: 1555.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T09:30:02.357674Z",
     "start_time": "2024-04-25T09:30:02.339671Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
