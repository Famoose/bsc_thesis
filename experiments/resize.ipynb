{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:55:10.461207Z",
     "start_time": "2024-04-24T20:55:10.457605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Param\n",
    "TOTAL_TIMESTEP_NUMB = 800_000\n",
    "LEARNING_RATE = 0.0001\n",
    "GAE = 1.0\n",
    "ENT_COEF = 0.01\n",
    "N_STEPS = 512\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Test Param\n",
    "EVAL_FREQ = 10000\n",
    "TEST_EPISODE_NUMBERS = 20"
   ],
   "id": "190e8d813e519441",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:55:10.469152Z",
     "start_time": "2024-04-24T20:55:10.466898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path('./resize')"
   ],
   "id": "356c5b9f625e9698",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:55:10.473762Z",
     "start_time": "2024-04-24T20:55:10.470961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model_cnn import MarioNet\n",
    "\n",
    "policy_kwargs_84 = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")\n",
    "policy_kwargs_42 = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "policy_kwargs_21 = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    ")"
   ],
   "id": "5e5076bbc86f9dfc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:55:21.771575Z",
     "start_time": "2024-04-24T20:55:10.475008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import make_parallel_env, STAGE_RECTANGLE\n",
    "\n",
    "env_84 = make_parallel_env(STAGE_RECTANGLE, 8)\n",
    "env_42 = make_parallel_env(STAGE_RECTANGLE, 8, resize=42)\n",
    "env_21 = make_parallel_env(STAGE_RECTANGLE, 8, resize=21)\n",
    "\n",
    "# dict with the different environments and names\n",
    "models = {\n",
    "    'resize_84': dict(env=env_84, policy_kwargs=policy_kwargs_84, eval=make_parallel_env(STAGE_RECTANGLE, 1)),\n",
    "    'resize_42': dict(env=env_42, policy_kwargs=policy_kwargs_42, eval=make_parallel_env(STAGE_RECTANGLE, 1, resize=42)),\n",
    "    'resize_21': dict(env=env_21, policy_kwargs=policy_kwargs_21, eval=make_parallel_env(STAGE_RECTANGLE, 1, resize=21))\n",
    "}"
   ],
   "id": "ae5282df420a1cc",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T21:17:22.981213Z",
     "start_time": "2024-04-24T20:55:21.772381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import TrainAndLoggingCallback\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "for key in models.keys():\n",
    "    print(f\"Training {key}\")\n",
    "    model = PPO('CnnPolicy', models[key]['env'], verbose=0, policy_kwargs=models[key]['policy_kwargs'], tensorboard_log=save_dir,\n",
    "                learning_rate=LEARNING_RATE, n_steps=N_STEPS, batch_size=BATCH_SIZE, n_epochs=N_EPOCHS, gamma=GAMMA, gae_lambda=GAE, ent_coef=ENT_COEF)\n",
    "    callback = TrainAndLoggingCallback(test_env=models[key]['env'], check_freq=EVAL_FREQ, episode_num=TEST_EPISODE_NUMBERS)\n",
    "    \n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, tb_log_name=key, callback=callback)\n",
    "    \n",
    "    "
   ],
   "id": "b979069e0aa609ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resize_84\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m model \u001B[38;5;241m=\u001B[39m PPO(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCnnPolicy\u001B[39m\u001B[38;5;124m'\u001B[39m, models[key][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menv\u001B[39m\u001B[38;5;124m'\u001B[39m], verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, policy_kwargs\u001B[38;5;241m=\u001B[39mmodels[key][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpolicy_kwargs\u001B[39m\u001B[38;5;124m'\u001B[39m], tensorboard_log\u001B[38;5;241m=\u001B[39msave_dir,\n\u001B[1;32m      7\u001B[0m             learning_rate\u001B[38;5;241m=\u001B[39mLEARNING_RATE, n_steps\u001B[38;5;241m=\u001B[39mN_STEPS, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, n_epochs\u001B[38;5;241m=\u001B[39mN_EPOCHS, gamma\u001B[38;5;241m=\u001B[39mGAMMA, gae_lambda\u001B[38;5;241m=\u001B[39mGAE, ent_coef\u001B[38;5;241m=\u001B[39mENT_COEF)\n\u001B[1;32m      8\u001B[0m callback \u001B[38;5;241m=\u001B[39m TrainAndLoggingCallback(test_env\u001B[38;5;241m=\u001B[39mmodels[key][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menv\u001B[39m\u001B[38;5;124m'\u001B[39m], check_freq\u001B[38;5;241m=\u001B[39mEVAL_FREQ, episode_num\u001B[38;5;241m=\u001B[39mTEST_EPISODE_NUMBERS)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTOTAL_TIMESTEP_NUMB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:310\u001B[0m, in \u001B[0;36mPPO.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    299\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    307\u001B[0m     reset_num_timesteps: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    308\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPPO\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_log_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_log_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:247\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001B[0m\n\u001B[1;32m    243\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_start(\u001B[38;5;28mlocals\u001B[39m(), \u001B[38;5;28mglobals\u001B[39m())\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[0;32m--> 247\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m continue_training \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:181\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n\u001B[1;32m    180\u001B[0m callback\u001B[38;5;241m.\u001B[39mupdate_locals(\u001B[38;5;28mlocals\u001B[39m())\n\u001B[0;32m--> 181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_info_buffer(infos)\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:88\u001B[0m, in \u001B[0;36mBaseCallback.on_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# timesteps start at zero\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnum_timesteps\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/utils.py:141\u001B[0m, in \u001B[0;36mTrainAndLoggingCallback._on_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    139\u001B[0m total_reward[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    140\u001B[0m total_time[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 141\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done \u001B[38;5;129;01mand\u001B[39;00m total_time[i] \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMAX_TIMESTEP_TEST:\n\u001B[1;32m    142\u001B[0m     action, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpredict(state)\n\u001B[1;32m    143\u001B[0m     state, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T21:17:22.982340Z",
     "start_time": "2024-04-24T21:17:22.982280Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
