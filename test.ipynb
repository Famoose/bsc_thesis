{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-18T21:15:25.995513Z",
     "start_time": "2024-04-18T21:15:25.993818Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:32:20.111041Z",
     "start_time": "2024-04-19T12:32:20.108532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#get num cpu\n",
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ],
   "id": "c4c13aa5b2ad9d86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:47:54.377803Z",
     "start_time": "2024-04-19T12:42:24.065206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "cpus = [10, 8, 6, 4, 2, 1]\n",
    "# Parallel environments\n",
    "for cpu in cpus:\n",
    "    vec_env = make_vec_env(\"CartPole-v1\", n_envs=cpu)\n",
    "    eval = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "    \n",
    "    model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./ppo_cartpole_tensorboard/\")\n",
    "    model.learn(total_timesteps=150_000, tb_log_name=\"cpu_\" + str(cpu) , eval_env=eval, eval_freq=1000, n_eval_episodes=10)\n",
    "    \n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "#mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
    "#print(f\"Mean reward: {mean_reward} +/- {std_reward:.2f}\")"
   ],
   "id": "5d59f648c848170b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/cpu_10_1\n",
      "Eval num_timesteps=10000, episode_reward=105.70 +/- 31.76\n",
      "Episode length: 105.70 +/- 31.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 106      |\n",
      "|    mean_reward     | 106      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=124.90 +/- 34.22\n",
      "Episode length: 124.90 +/- 34.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 125      |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.6     |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 30289    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=386.50 +/- 93.90\n",
      "Episode length: 386.50 +/- 93.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 386         |\n",
      "|    mean_reward          | 386         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015345764 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | -0.00201    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=410.50 +/- 119.33\n",
      "Episode length: 410.50 +/- 119.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 410      |\n",
      "|    mean_reward     | 410      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 36.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 8255     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=399.40 +/- 102.70\n",
      "Episode length: 399.40 +/- 102.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 399         |\n",
      "|    mean_reward          | 399         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019784803 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 27.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=358.50 +/- 104.87\n",
      "Episode length: 358.50 +/- 104.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 358      |\n",
      "|    mean_reward     | 358      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.1     |\n",
      "|    ep_rew_mean     | 86.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 6877     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=377.80 +/- 108.39\n",
      "Episode length: 377.80 +/- 108.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 378         |\n",
      "|    mean_reward          | 378         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012662542 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 37.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=349.50 +/- 106.78\n",
      "Episode length: 349.50 +/- 106.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 350      |\n",
      "|    mean_reward     | 350      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 179      |\n",
      "|    ep_rew_mean     | 179      |\n",
      "| time/              |          |\n",
      "|    fps             | 6351     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=421.10 +/- 96.51\n",
      "Episode length: 421.10 +/- 96.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 421         |\n",
      "|    mean_reward          | 421         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012464427 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.76        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=463.70 +/- 66.63\n",
      "Episode length: 463.70 +/- 66.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 464      |\n",
      "|    mean_reward     | 464      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 257      |\n",
      "|    ep_rew_mean     | 257      |\n",
      "| time/              |          |\n",
      "|    fps             | 6026     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=476.40 +/- 61.32\n",
      "Episode length: 476.40 +/- 61.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 476         |\n",
      "|    mean_reward          | 476         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010193875 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00961    |\n",
      "|    value_loss           | 9.88        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=431.60 +/- 85.99\n",
      "Episode length: 431.60 +/- 85.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 432      |\n",
      "|    mean_reward     | 432      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 307      |\n",
      "|    ep_rew_mean     | 307      |\n",
      "| time/              |          |\n",
      "|    fps             | 5827     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=485.90 +/- 25.92\n",
      "Episode length: 485.90 +/- 25.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 486         |\n",
      "|    mean_reward          | 486         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007896049 |\n",
      "|    clip_fraction        | 0.0902      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.84        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0082     |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=485.10 +/- 44.70\n",
      "Episode length: 485.10 +/- 44.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 485      |\n",
      "|    mean_reward     | 485      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 384      |\n",
      "|    ep_rew_mean     | 384      |\n",
      "| time/              |          |\n",
      "|    fps             | 5670     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007851964 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.301       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00933    |\n",
      "|    value_loss           | 1.89        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 443      |\n",
      "|    ep_rew_mean     | 443      |\n",
      "| time/              |          |\n",
      "|    fps             | 5557     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/cpu_8_1\n",
      "Eval num_timesteps=8000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 11641    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=442.90 +/- 105.26\n",
      "Episode length: 442.90 +/- 105.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 443         |\n",
      "|    mean_reward          | 443         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015347777 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.00174     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.74        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 13.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=412.80 +/- 121.12\n",
      "Episode length: 412.80 +/- 121.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 413      |\n",
      "|    mean_reward     | 413      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.7     |\n",
      "|    ep_rew_mean     | 37.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 6717     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=375.00 +/- 111.35\n",
      "Episode length: 375.00 +/- 111.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 375         |\n",
      "|    mean_reward          | 375         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019323837 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=374.20 +/- 81.96\n",
      "Episode length: 374.20 +/- 81.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.6     |\n",
      "|    ep_rew_mean     | 94.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 5949     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=412.50 +/- 93.15\n",
      "Episode length: 412.50 +/- 93.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 412         |\n",
      "|    mean_reward          | 412         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012384269 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 39.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=382.80 +/- 95.31\n",
      "Episode length: 382.80 +/- 95.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | 383      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 169      |\n",
      "|    ep_rew_mean     | 169      |\n",
      "| time/              |          |\n",
      "|    fps             | 5606     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=483.00 +/- 51.00\n",
      "Episode length: 483.00 +/- 51.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017372668 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.62        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=471.00 +/- 58.50\n",
      "Episode length: 471.00 +/- 58.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 242      |\n",
      "|    ep_rew_mean     | 242      |\n",
      "| time/              |          |\n",
      "|    fps             | 5358     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=479.30 +/- 33.21\n",
      "Episode length: 479.30 +/- 33.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 479         |\n",
      "|    mean_reward          | 479         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013378635 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    value_loss           | 5.02        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=473.20 +/- 43.37\n",
      "Episode length: 473.20 +/- 43.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 473      |\n",
      "|    mean_reward     | 473      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 332      |\n",
      "|    ep_rew_mean     | 332      |\n",
      "| time/              |          |\n",
      "|    fps             | 5212     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=497.90 +/- 6.30\n",
      "Episode length: 497.90 +/- 6.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 498         |\n",
      "|    mean_reward          | 498         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 104000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007162614 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=494.00 +/- 12.21\n",
      "Episode length: 494.00 +/- 12.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 403      |\n",
      "|    ep_rew_mean     | 403      |\n",
      "| time/              |          |\n",
      "|    fps             | 5101     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071920604 |\n",
      "|    clip_fraction        | 0.0773       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00834     |\n",
      "|    value_loss           | 1.85         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 467      |\n",
      "|    ep_rew_mean     | 467      |\n",
      "| time/              |          |\n",
      "|    fps             | 5023     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033087097 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | 0.957        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.172        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    value_loss           | 1.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 492      |\n",
      "|    ep_rew_mean     | 492      |\n",
      "| time/              |          |\n",
      "|    fps             | 4963     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037258782 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.436       |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0161       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00392     |\n",
      "|    value_loss           | 0.773        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 499      |\n",
      "|    ep_rew_mean     | 499      |\n",
      "| time/              |          |\n",
      "|    fps             | 4885     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/cpu_6_1\n",
      "Eval num_timesteps=6000, episode_reward=9.30 +/- 0.78\n",
      "Episode length: 9.30 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.3      |\n",
      "|    mean_reward     | 9.3      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=9.60 +/- 0.66\n",
      "Episode length: 9.60 +/- 0.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.6     |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 31970    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=290.10 +/- 90.60\n",
      "Episode length: 290.10 +/- 90.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 290         |\n",
      "|    mean_reward          | 290         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014932712 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.00137    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.6         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 16.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=296.40 +/- 111.98\n",
      "Episode length: 296.40 +/- 111.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33.6     |\n",
      "|    ep_rew_mean     | 33.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 8172     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=316.90 +/- 116.09\n",
      "Episode length: 316.90 +/- 116.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 317         |\n",
      "|    mean_reward          | 317         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019088069 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0393     |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=362.30 +/- 100.28\n",
      "Episode length: 362.30 +/- 100.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 362      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.8     |\n",
      "|    ep_rew_mean     | 66.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 6448     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=370.80 +/- 94.17\n",
      "Episode length: 370.80 +/- 94.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 371        |\n",
      "|    mean_reward          | 371        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 42000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01427535 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.599     |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.3       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    value_loss           | 44.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=374.10 +/- 94.78\n",
      "Episode length: 374.10 +/- 94.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 133      |\n",
      "|    ep_rew_mean     | 133      |\n",
      "| time/              |          |\n",
      "|    fps             | 5780     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=420.40 +/- 79.30\n",
      "Episode length: 420.40 +/- 79.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 420         |\n",
      "|    mean_reward          | 420         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013579224 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=430.60 +/- 94.76\n",
      "Episode length: 430.60 +/- 94.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 431      |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 192      |\n",
      "|    ep_rew_mean     | 192      |\n",
      "| time/              |          |\n",
      "|    fps             | 5398     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=414.40 +/- 85.14\n",
      "Episode length: 414.40 +/- 85.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 414         |\n",
      "|    mean_reward          | 414         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011612251 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.3         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=399.40 +/- 102.34\n",
      "Episode length: 399.40 +/- 102.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 399      |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 252      |\n",
      "|    ep_rew_mean     | 252      |\n",
      "| time/              |          |\n",
      "|    fps             | 5182     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=427.60 +/- 73.36\n",
      "Episode length: 427.60 +/- 73.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 428         |\n",
      "|    mean_reward          | 428         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 78000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009392378 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 8.68        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=468.70 +/- 63.71\n",
      "Episode length: 468.70 +/- 63.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 469      |\n",
      "|    mean_reward     | 469      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 310      |\n",
      "|    ep_rew_mean     | 310      |\n",
      "| time/              |          |\n",
      "|    fps             | 5009     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=494.80 +/- 15.60\n",
      "Episode length: 494.80 +/- 15.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | 495         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008702253 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.15        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 5.53        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=487.10 +/- 30.06\n",
      "Episode length: 487.10 +/- 30.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 487      |\n",
      "|    mean_reward     | 487      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 362      |\n",
      "|    ep_rew_mean     | 362      |\n",
      "| time/              |          |\n",
      "|    fps             | 4869     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 102000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009511893 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 411      |\n",
      "|    ep_rew_mean     | 411      |\n",
      "| time/              |          |\n",
      "|    fps             | 4763     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 114000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066386745 |\n",
      "|    clip_fraction        | 0.0796       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.472       |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.105        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    value_loss           | 1.52         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 453      |\n",
      "|    ep_rew_mean     | 453      |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 126000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037895131 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.218        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    value_loss           | 0.626        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 483      |\n",
      "|    ep_rew_mean     | 483      |\n",
      "| time/              |          |\n",
      "|    fps             | 4617     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 138000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029767586 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.451       |\n",
      "|    explained_variance   | 0.401        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0274       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 0.456        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 498      |\n",
      "|    ep_rew_mean     | 498      |\n",
      "| time/              |          |\n",
      "|    fps             | 4568     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044580535 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0241       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 0.312        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 156000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 4529     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/cpu_4_1\n",
      "Eval num_timesteps=4000, episode_reward=44.70 +/- 7.18\n",
      "Episode length: 44.70 +/- 7.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 44.7     |\n",
      "|    mean_reward     | 44.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=47.80 +/- 10.20\n",
      "Episode length: 47.80 +/- 10.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 47.8     |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 19081    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=262.40 +/- 161.46\n",
      "Episode length: 262.40 +/- 161.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 262         |\n",
      "|    mean_reward          | 262         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013621896 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.00586     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.5         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 19          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=244.50 +/- 132.80\n",
      "Episode length: 244.50 +/- 132.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 244      |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.1     |\n",
      "|    ep_rew_mean     | 37.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 6824     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=309.80 +/- 98.07\n",
      "Episode length: 309.80 +/- 98.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 310         |\n",
      "|    mean_reward          | 310         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014628815 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 26.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=330.30 +/- 168.51\n",
      "Episode length: 330.30 +/- 168.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 330      |\n",
      "|    mean_reward     | 330      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 61.1     |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 5441     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=375.90 +/- 108.02\n",
      "Episode length: 375.90 +/- 108.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 376         |\n",
      "|    mean_reward          | 376         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014907116 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=371.80 +/- 121.02\n",
      "Episode length: 371.80 +/- 121.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 372      |\n",
      "|    mean_reward     | 372      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 103      |\n",
      "|    ep_rew_mean     | 103      |\n",
      "| time/              |          |\n",
      "|    fps             | 4863     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=458.20 +/- 73.53\n",
      "Episode length: 458.20 +/- 73.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 458         |\n",
      "|    mean_reward          | 458         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011916173 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.51        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 38.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=404.70 +/- 101.59\n",
      "Episode length: 404.70 +/- 101.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 405      |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 160      |\n",
      "|    ep_rew_mean     | 160      |\n",
      "| time/              |          |\n",
      "|    fps             | 4530     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=453.50 +/- 76.07\n",
      "Episode length: 453.50 +/- 76.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 454         |\n",
      "|    mean_reward          | 454         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010754241 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.21        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=453.70 +/- 76.69\n",
      "Episode length: 453.70 +/- 76.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 454      |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 207      |\n",
      "|    ep_rew_mean     | 207      |\n",
      "| time/              |          |\n",
      "|    fps             | 4315     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=474.00 +/- 45.23\n",
      "Episode length: 474.00 +/- 45.23\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 474       |\n",
      "|    mean_reward          | 474       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 52000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0091547 |\n",
      "|    clip_fraction        | 0.113     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.541    |\n",
      "|    explained_variance   | 0.875     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.51      |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -0.0129   |\n",
      "|    value_loss           | 14.2      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=471.40 +/- 57.21\n",
      "Episode length: 471.40 +/- 57.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 259      |\n",
      "|    ep_rew_mean     | 259      |\n",
      "| time/              |          |\n",
      "|    fps             | 4160     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009367781 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.05        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 11.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 317      |\n",
      "|    ep_rew_mean     | 317      |\n",
      "| time/              |          |\n",
      "|    fps             | 4039     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008718206 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.759       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 364      |\n",
      "|    ep_rew_mean     | 364      |\n",
      "| time/              |          |\n",
      "|    fps             | 3934     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046273195 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.201        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0035      |\n",
      "|    value_loss           | 1.25         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 409      |\n",
      "|    ep_rew_mean     | 409      |\n",
      "| time/              |          |\n",
      "|    fps             | 3869     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045912517 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.466       |\n",
      "|    explained_variance   | 0.274        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0782       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 0.992        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 450      |\n",
      "|    ep_rew_mean     | 450      |\n",
      "| time/              |          |\n",
      "|    fps             | 3810     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040381923 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | 0.0391       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.16         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000877    |\n",
      "|    value_loss           | 0.657        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 479      |\n",
      "|    ep_rew_mean     | 479      |\n",
      "| time/              |          |\n",
      "|    fps             | 3765     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044750916 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.465       |\n",
      "|    explained_variance   | 0.00492      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000309    |\n",
      "|    value_loss           | 0.452        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 498      |\n",
      "|    ep_rew_mean     | 498      |\n",
      "| time/              |          |\n",
      "|    fps             | 3727     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030341095 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | 0.01         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.113        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000415    |\n",
      "|    value_loss           | 0.286        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 3700     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045839325 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | 0.0601       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0222       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 0.177        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 3678     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053224615 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | 0.0334       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00213      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    value_loss           | 0.113        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 3659     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017306875 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | -0.00919     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.026        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000446    |\n",
      "|    value_loss           | 0.0708       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 3642     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013867313 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | -0.0069      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0126       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000405    |\n",
      "|    value_loss           | 0.0446       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 3626     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042972984 |\n",
      "|    clip_fraction        | 0.046        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | -0.0103      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00713      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 0.0314       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 3612     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/cpu_2_1\n",
      "Eval num_timesteps=2000, episode_reward=9.10 +/- 0.70\n",
      "Episode length: 9.10 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.1      |\n",
      "|    mean_reward     | 9.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=9.60 +/- 0.66\n",
      "Episode length: 9.60 +/- 0.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.5     |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 12440    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=89.60 +/- 22.15\n",
      "Episode length: 89.60 +/- 22.15\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 89.6       |\n",
      "|    mean_reward          | 89.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01409282 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.683     |\n",
      "|    explained_variance   | -0.00332   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.11       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    value_loss           | 27.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=220.30 +/- 189.14\n",
      "Episode length: 220.30 +/- 189.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 220      |\n",
      "|    mean_reward     | 220      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 30.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 5366     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=286.20 +/- 161.26\n",
      "Episode length: 286.20 +/- 161.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 286        |\n",
      "|    mean_reward          | 286        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00996884 |\n",
      "|    clip_fraction        | 0.0993     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.653     |\n",
      "|    explained_variance   | 0.227      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.1       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 32         |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=318.70 +/- 178.69\n",
      "Episode length: 318.70 +/- 178.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 319      |\n",
      "|    mean_reward     | 319      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46.1     |\n",
      "|    ep_rew_mean     | 46.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 4078     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=390.60 +/- 151.87\n",
      "Episode length: 390.60 +/- 151.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 391         |\n",
      "|    mean_reward          | 391         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010031222 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=340.20 +/- 138.59\n",
      "Episode length: 340.20 +/- 138.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 340      |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 62.7     |\n",
      "|    ep_rew_mean     | 62.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3529     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=362.80 +/- 96.95\n",
      "Episode length: 362.80 +/- 96.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 363         |\n",
      "|    mean_reward          | 363         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012259425 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 51.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=352.00 +/- 122.51\n",
      "Episode length: 352.00 +/- 122.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 352      |\n",
      "|    mean_reward     | 352      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.1     |\n",
      "|    ep_rew_mean     | 90.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3291     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=319.10 +/- 90.97\n",
      "Episode length: 319.10 +/- 90.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | 319          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096523855 |\n",
      "|    clip_fraction        | 0.088        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.542        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.6         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0123      |\n",
      "|    value_loss           | 36           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=405.40 +/- 94.21\n",
      "Episode length: 405.40 +/- 94.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 405      |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | 122      |\n",
      "| time/              |          |\n",
      "|    fps             | 3146     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=399.80 +/- 97.99\n",
      "Episode length: 399.80 +/- 97.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 400         |\n",
      "|    mean_reward          | 400         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009543574 |\n",
      "|    clip_fraction        | 0.0974      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.15        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 21.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=419.70 +/- 107.32\n",
      "Episode length: 419.70 +/- 107.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 420      |\n",
      "|    mean_reward     | 420      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 150      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 2992     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=467.40 +/- 65.54\n",
      "Episode length: 467.40 +/- 65.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 467         |\n",
      "|    mean_reward          | 467         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007953077 |\n",
      "|    clip_fraction        | 0.0859      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=426.10 +/- 75.74\n",
      "Episode length: 426.10 +/- 75.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 426      |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 186      |\n",
      "|    ep_rew_mean     | 186      |\n",
      "| time/              |          |\n",
      "|    fps             | 2890     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=491.20 +/- 14.79\n",
      "Episode length: 491.20 +/- 14.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 491          |\n",
      "|    mean_reward          | 491          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058917794 |\n",
      "|    clip_fraction        | 0.0664       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.55        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.04         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0119      |\n",
      "|    value_loss           | 17.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=488.20 +/- 20.00\n",
      "Episode length: 488.20 +/- 20.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 488      |\n",
      "|    mean_reward     | 488      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 214      |\n",
      "|    ep_rew_mean     | 214      |\n",
      "| time/              |          |\n",
      "|    fps             | 2792     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007351584 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 8.88        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 248      |\n",
      "|    ep_rew_mean     | 248      |\n",
      "| time/              |          |\n",
      "|    fps             | 2717     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004127262 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.298       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 282      |\n",
      "|    ep_rew_mean     | 282      |\n",
      "| time/              |          |\n",
      "|    fps             | 2659     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005330545 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.359       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 312      |\n",
      "|    ep_rew_mean     | 312      |\n",
      "| time/              |          |\n",
      "|    fps             | 2615     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004092571 |\n",
      "|    clip_fraction        | 0.0281      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.494      |\n",
      "|    explained_variance   | 0.0764      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 340      |\n",
      "|    ep_rew_mean     | 340      |\n",
      "| time/              |          |\n",
      "|    fps             | 2577     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019791021 |\n",
      "|    clip_fraction        | 0.0137       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.00922      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0374       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000877    |\n",
      "|    value_loss           | 0.601        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 370      |\n",
      "|    ep_rew_mean     | 370      |\n",
      "| time/              |          |\n",
      "|    fps             | 2546     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005488162 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.022       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.089       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 0.415       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 402      |\n",
      "| time/              |          |\n",
      "|    fps             | 2518     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 62000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056424593 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.486       |\n",
      "|    explained_variance   | -0.00389     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.117        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 0.255        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 424      |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    fps             | 2495     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 66000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00288436 |\n",
      "|    clip_fraction        | 0.0161     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.501     |\n",
      "|    explained_variance   | -0.00581   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0415     |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.000315  |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 450      |\n",
      "|    ep_rew_mean     | 450      |\n",
      "| time/              |          |\n",
      "|    fps             | 2475     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049177874 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.496       |\n",
      "|    explained_variance   | -0.00934     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0224       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 0.0996       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 468      |\n",
      "|    ep_rew_mean     | 468      |\n",
      "| time/              |          |\n",
      "|    fps             | 2460     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004937577 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.0282      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00377     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    value_loss           | 0.0626      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 484      |\n",
      "|    ep_rew_mean     | 484      |\n",
      "| time/              |          |\n",
      "|    fps             | 2444     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064735934 |\n",
      "|    clip_fraction        | 0.0441       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | -0.0218      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00344      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 0.037        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 496      |\n",
      "|    ep_rew_mean     | 496      |\n",
      "| time/              |          |\n",
      "|    fps             | 2429     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 82000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024289805 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | 0.0145       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0109       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 0.0257       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2373     |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006870023 |\n",
      "|    clip_fraction        | 0.00676      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.482       |\n",
      "|    explained_variance   | 0.00498      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000844     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | 0.000214     |\n",
      "|    value_loss           | 0.0165       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2365     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040068952 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | -0.00354     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0176      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00039     |\n",
      "|    value_loss           | 0.0106       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2359     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004522182 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | 0.0109      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00762     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    value_loss           | 0.00715     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2352     |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002141322 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | -0.00594    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0176      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.000159   |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2345     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 104000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039183386 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | -0.0122      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000933    |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000557    |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2339     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 108000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050317775 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0105       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0211       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 0.00222      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2333     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 112000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013072048 |\n",
      "|    clip_fraction        | 0.00981      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.0281       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000149     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | 0.000667     |\n",
      "|    value_loss           | 0.00149      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2328     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 116000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003451073 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.446      |\n",
      "|    explained_variance   | 0.0658      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00705     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2324     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020790983 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.0823       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0013       |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000693    |\n",
      "|    value_loss           | 0.000732     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2320     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055592405 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.448       |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00272     |\n",
      "|    value_loss           | 0.000522     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2317     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 128000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008163741 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.0503      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    value_loss           | 0.000368    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2313     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067415875 |\n",
      "|    clip_fraction        | 0.0934       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | 0.0349       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000742    |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 0.000275     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2309     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046315636 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.456       |\n",
      "|    explained_variance   | -0.0246      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000635     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    value_loss           | 0.000164     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2306     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041119778 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | -0.0583      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00848     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000796    |\n",
      "|    value_loss           | 0.000132     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2304     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004591123 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.452      |\n",
      "|    explained_variance   | 0.00753     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00241     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 9.14e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2300     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018998758 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.453       |\n",
      "|    explained_variance   | -0.0435      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.019        |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000814    |\n",
      "|    value_loss           | 6.21e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 2297     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/cpu_1_1\n",
      "Eval num_timesteps=1000, episode_reward=9.80 +/- 0.40\n",
      "Episode length: 9.80 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 9.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=9.70 +/- 0.78\n",
      "Episode length: 9.70 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 9.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 6697     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=324.30 +/- 163.66\n",
      "Episode length: 324.30 +/- 163.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | 324         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009118238 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00902    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.09        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 49.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=261.50 +/- 141.73\n",
      "Episode length: 261.50 +/- 141.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 262      |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.8     |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2786     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=340.80 +/- 167.71\n",
      "Episode length: 340.80 +/- 167.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 341         |\n",
      "|    mean_reward          | 341         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009881334 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.0867      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 38.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=373.40 +/- 119.32\n",
      "Episode length: 373.40 +/- 119.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 373      |\n",
      "|    mean_reward     | 373      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 34.9     |\n",
      "|    ep_rew_mean     | 34.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 2228     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=168.20 +/- 117.79\n",
      "Episode length: 168.20 +/- 117.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 168          |\n",
      "|    mean_reward          | 168          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095062675 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.63        |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0214      |\n",
      "|    value_loss           | 52.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=156.20 +/- 85.42\n",
      "Episode length: 156.20 +/- 85.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 156      |\n",
      "|    mean_reward     | 156      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46       |\n",
      "|    ep_rew_mean     | 46       |\n",
      "| time/              |          |\n",
      "|    fps             | 2234     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=285.90 +/- 153.22\n",
      "Episode length: 285.90 +/- 153.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 286         |\n",
      "|    mean_reward          | 286         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008107676 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=277.50 +/- 162.02\n",
      "Episode length: 277.50 +/- 162.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 278      |\n",
      "|    mean_reward     | 278      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58.8     |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2134     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=357.50 +/- 109.74\n",
      "Episode length: 357.50 +/- 109.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | 358          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068658283 |\n",
      "|    clip_fraction        | 0.0817       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.601       |\n",
      "|    explained_variance   | 0.54         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0161      |\n",
      "|    value_loss           | 54.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=323.20 +/- 123.59\n",
      "Episode length: 323.20 +/- 123.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 323      |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.6     |\n",
      "|    ep_rew_mean     | 74.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2030     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=418.50 +/- 105.47\n",
      "Episode length: 418.50 +/- 105.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 418          |\n",
      "|    mean_reward          | 418          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119580235 |\n",
      "|    clip_fraction        | 0.0911       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 47.5         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=432.90 +/- 90.83\n",
      "Episode length: 432.90 +/- 90.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 433      |\n",
      "|    mean_reward     | 433      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.4     |\n",
      "|    ep_rew_mean     | 91.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 1917     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=461.50 +/- 77.17\n",
      "Episode length: 461.50 +/- 77.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 462         |\n",
      "|    mean_reward          | 462         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004381204 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0074     |\n",
      "|    value_loss           | 53.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=418.60 +/- 82.05\n",
      "Episode length: 418.60 +/- 82.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 419      |\n",
      "|    mean_reward     | 419      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 1831     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=485.90 +/- 34.77\n",
      "Episode length: 485.90 +/- 34.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 486          |\n",
      "|    mean_reward          | 486          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032679155 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    value_loss           | 23.6         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=469.20 +/- 38.14\n",
      "Episode length: 469.20 +/- 38.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 469      |\n",
      "|    mean_reward     | 469      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 127      |\n",
      "| time/              |          |\n",
      "|    fps             | 1759     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005019081 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    value_loss           | 64.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 146      |\n",
      "|    ep_rew_mean     | 146      |\n",
      "| time/              |          |\n",
      "|    fps             | 1699     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 21000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025843363 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.543        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.57         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 35           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 160      |\n",
      "|    ep_rew_mean     | 160      |\n",
      "| time/              |          |\n",
      "|    fps             | 1653     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006215037 |\n",
      "|    clip_fraction        | 0.0481      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.00734     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56          |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00617    |\n",
      "|    value_loss           | 60.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 183      |\n",
      "|    ep_rew_mean     | 183      |\n",
      "| time/              |          |\n",
      "|    fps             | 1615     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=482.90 +/- 26.81\n",
      "Episode length: 482.90 +/- 26.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007280346 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | -0.0241     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.294       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00497    |\n",
      "|    value_loss           | 5.53        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=445.10 +/- 57.41\n",
      "Episode length: 445.10 +/- 57.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 445      |\n",
      "|    mean_reward     | 445      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 199      |\n",
      "|    ep_rew_mean     | 199      |\n",
      "| time/              |          |\n",
      "|    fps             | 1592     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=493.40 +/- 19.80\n",
      "Episode length: 493.40 +/- 19.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 493         |\n",
      "|    mean_reward          | 493         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005208441 |\n",
      "|    clip_fraction        | 0.0301      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.41        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 9.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=493.90 +/- 18.30\n",
      "Episode length: 493.90 +/- 18.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 215      |\n",
      "|    ep_rew_mean     | 215      |\n",
      "| time/              |          |\n",
      "|    fps             | 1568     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050728247 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00273     |\n",
      "|    value_loss           | 22           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 232      |\n",
      "|    ep_rew_mean     | 232      |\n",
      "| time/              |          |\n",
      "|    fps             | 1547     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005090602 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.504      |\n",
      "|    explained_variance   | 0.0201      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 255      |\n",
      "|    ep_rew_mean     | 255      |\n",
      "| time/              |          |\n",
      "|    fps             | 1530     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033323853 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.344        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.13         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00488     |\n",
      "|    value_loss           | 0.949        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 272      |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    fps             | 1515     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048288354 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | -0.0268      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0359       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    value_loss           | 0.538        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 290      |\n",
      "|    ep_rew_mean     | 290      |\n",
      "| time/              |          |\n",
      "|    fps             | 1501     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013991375 |\n",
      "|    clip_fraction        | 0.00356      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | -0.0355      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00166      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.000547     |\n",
      "|    value_loss           | 0.349        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 307      |\n",
      "|    ep_rew_mean     | 307      |\n",
      "| time/              |          |\n",
      "|    fps             | 1490     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031181313 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | -0.00797     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0189       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 0.218        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 324      |\n",
      "|    ep_rew_mean     | 324      |\n",
      "| time/              |          |\n",
      "|    fps             | 1475     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013258122 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.531       |\n",
      "|    explained_variance   | -0.0076      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0203       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000103    |\n",
      "|    value_loss           | 0.138        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 339      |\n",
      "|    ep_rew_mean     | 339      |\n",
      "| time/              |          |\n",
      "|    fps             | 1438     |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008412547 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | -0.0192      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00919      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000312    |\n",
      "|    value_loss           | 0.0884       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 355      |\n",
      "|    ep_rew_mean     | 355      |\n",
      "| time/              |          |\n",
      "|    fps             | 1432     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005960188 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | -0.0427     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00334    |\n",
      "|    value_loss           | 0.0524      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 371      |\n",
      "|    ep_rew_mean     | 371      |\n",
      "| time/              |          |\n",
      "|    fps             | 1426     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055714296 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.516       |\n",
      "|    explained_variance   | 0.0111       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.23e-05     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    value_loss           | 0.0331       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 386      |\n",
      "|    ep_rew_mean     | 386      |\n",
      "| time/              |          |\n",
      "|    fps             | 1420     |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055727013 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.508       |\n",
      "|    explained_variance   | -0.018       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00767     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 0.0212       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 399      |\n",
      "|    ep_rew_mean     | 399      |\n",
      "| time/              |          |\n",
      "|    fps             | 1413     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047938763 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.00709      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0033      |\n",
      "|    value_loss           | 0.0134       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 416      |\n",
      "|    ep_rew_mean     | 416      |\n",
      "| time/              |          |\n",
      "|    fps             | 1408     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039420757 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.516       |\n",
      "|    explained_variance   | 0.0554       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0261      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    value_loss           | 0.0093       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 430      |\n",
      "|    ep_rew_mean     | 430      |\n",
      "| time/              |          |\n",
      "|    fps             | 1404     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006307503 |\n",
      "|    clip_fraction        | 0.0803      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | -0.021      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0169      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    value_loss           | 0.00604     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 442      |\n",
      "|    ep_rew_mean     | 442      |\n",
      "| time/              |          |\n",
      "|    fps             | 1400     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005855771 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.0472      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 455      |\n",
      "|    ep_rew_mean     | 455      |\n",
      "| time/              |          |\n",
      "|    fps             | 1397     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003930863 |\n",
      "|    clip_fraction        | 0.0274      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | -0.00974    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00358    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 0.00259     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 461      |\n",
      "|    ep_rew_mean     | 461      |\n",
      "| time/              |          |\n",
      "|    fps             | 1393     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 62000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045438586 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | -0.0129      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00936      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000803    |\n",
      "|    value_loss           | 0.00175      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 468      |\n",
      "|    ep_rew_mean     | 468      |\n",
      "| time/              |          |\n",
      "|    fps             | 1390     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015051791 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.0217       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00155      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | 0.00011      |\n",
      "|    value_loss           | 0.00119      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 477      |\n",
      "|    ep_rew_mean     | 477      |\n",
      "| time/              |          |\n",
      "|    fps             | 1388     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 66000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027269863 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | -0.0245      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00764      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 0.000848     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 484      |\n",
      "|    ep_rew_mean     | 484      |\n",
      "| time/              |          |\n",
      "|    fps             | 1385     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054555046 |\n",
      "|    clip_fraction        | 0.0532       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | 0.00304      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00156     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 0.000542     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 488      |\n",
      "|    ep_rew_mean     | 488      |\n",
      "| time/              |          |\n",
      "|    fps             | 1383     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029339911 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.449       |\n",
      "|    explained_variance   | 0.097        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00108     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 0.000387     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 495      |\n",
      "|    ep_rew_mean     | 495      |\n",
      "| time/              |          |\n",
      "|    fps             | 1381     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041919034 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.0755       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0227      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    value_loss           | 0.000273     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 497      |\n",
      "|    ep_rew_mean     | 497      |\n",
      "| time/              |          |\n",
      "|    fps             | 1379     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002704263 |\n",
      "|    clip_fraction        | 0.0394      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | -0.025      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00403     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.000911   |\n",
      "|    value_loss           | 0.000192    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 499      |\n",
      "|    ep_rew_mean     | 499      |\n",
      "| time/              |          |\n",
      "|    fps             | 1377     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056729577 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | -0.0257      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00414      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    value_loss           | 0.00015      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1375     |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 78000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004291689 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | -0.0119     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.000963   |\n",
      "|    value_loss           | 0.000103    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1373     |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030911646 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.437       |\n",
      "|    explained_variance   | -0.0334      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0228       |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 7.23e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1371     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 82000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014045191 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.436       |\n",
      "|    explained_variance   | -0.132       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00443     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | 0.000161     |\n",
      "|    value_loss           | 4.63e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1369     |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041526947 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.434       |\n",
      "|    explained_variance   | -0.154       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0134       |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000822    |\n",
      "|    value_loss           | 4.13e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1357     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 87000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008651281 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | -0.197      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00561    |\n",
      "|    value_loss           | 2.83e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1356     |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 89000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002665245 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | -0.171      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00538     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    value_loss           | 1.96e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1354     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 91000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012464607 |\n",
      "|    clip_fraction        | 0.0521       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.415       |\n",
      "|    explained_variance   | -0.237       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0161      |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000862    |\n",
      "|    value_loss           | 1.37e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1353     |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 93000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036300714 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | -0.287       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00543     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000992    |\n",
      "|    value_loss           | 1.07e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1352     |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039168936 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | -0.33        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0045       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00051     |\n",
      "|    value_loss           | 1.11e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1351     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 97000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019354196 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | -0.44        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00492      |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000335    |\n",
      "|    value_loss           | 7.92e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1351     |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 99000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009279151 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | -0.645       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00781     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -8.69e-05    |\n",
      "|    value_loss           | 4.51e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1349     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 101000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041570114 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | -0.447       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00648     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00336     |\n",
      "|    value_loss           | 4.46e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1348     |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 103000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037414469 |\n",
      "|    clip_fraction        | 0.0253       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | -1.34        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000508    |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00019     |\n",
      "|    value_loss           | 3.54e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1347     |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 105000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00503808 |\n",
      "|    clip_fraction        | 0.0268     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.333     |\n",
      "|    explained_variance   | -1.5       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00235   |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.0013    |\n",
      "|    value_loss           | 1.87e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1346     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 107000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039575836 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.318       |\n",
      "|    explained_variance   | -2.19        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00582     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.000986    |\n",
      "|    value_loss           | 1.61e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1346     |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 109000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004713209 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | -1.17       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00751    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 2.51e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1345     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 111000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006241883 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | -0.55       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    value_loss           | 2.4e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1344     |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 113000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027203336 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | -2.26        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00696     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 5.19e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1343     |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 115000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045073843 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | -3.87        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00365      |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 9.46e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1343     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 117000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002214232 |\n",
      "|    clip_fraction        | 0.0153      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.282      |\n",
      "|    explained_variance   | -2.08       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000175    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    value_loss           | 4.96e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1341     |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 119000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018842056 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | -4.86        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00326     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 9.32e-05     |\n",
      "|    value_loss           | 1.32e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1340     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 121000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027972548 |\n",
      "|    clip_fraction        | 0.0202       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | -3.97        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00269     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 1.27e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1340     |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 123000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024467648 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | -6.3         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00415     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 1.28e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1339     |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 125000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020877926 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | -9.36        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00316     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 9.53e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1338     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 127000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019220394 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | -11.3        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000878     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.000457    |\n",
      "|    value_loss           | 4.09e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1330     |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003988224 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | -8.26       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000601   |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    value_loss           | 1.4e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1330     |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023812102 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.248       |\n",
      "|    explained_variance   | -14.5        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0013       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | 0.000242     |\n",
      "|    value_loss           | 8.7e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1330     |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 134000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018618051 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | -8.58        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00982     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.000613    |\n",
      "|    value_loss           | 4.91e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1329     |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008969555 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | -8.69        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00104      |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000547    |\n",
      "|    value_loss           | 7.84e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1328     |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 138000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015814309 |\n",
      "|    clip_fraction        | 0.011        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | -12.2        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00664      |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000711    |\n",
      "|    value_loss           | 6.2e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1328     |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025261047 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.262       |\n",
      "|    explained_variance   | -7.98        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00104     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | 0.000535     |\n",
      "|    value_loss           | 4.62e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1327     |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 142000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023233457 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | -5.88        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | 0.000195     |\n",
      "|    value_loss           | 2.36e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1327     |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010854499 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | -6.01        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00138     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | 0.000167     |\n",
      "|    value_loss           | 2.15e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1326     |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 146000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012821422 |\n",
      "|    clip_fraction        | 0.0192       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | -6.77        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00799     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 2.5e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1326     |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048849466 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.262       |\n",
      "|    explained_variance   | -2.75        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00134     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 3.06e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1326     |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013606894 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.245       |\n",
      "|    explained_variance   | -7.59        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00436     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000989    |\n",
      "|    value_loss           | 3.45e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    fps             | 1325     |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
