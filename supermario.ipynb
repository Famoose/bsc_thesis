{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 5152646,
     "sourceType": "datasetVersion",
     "datasetId": 2993870
    }
   ],
   "dockerImageVersionId": 30407,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# **Super Mario Bros. with Stable-Baseline3 PPO**\n\n",
   "metadata": {
    "id": "iezMhADyMDIi"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## First steps",
   "metadata": {
    "id": "iU7a5nPOUlK5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym_super_mario_bros\n",
    "#The 1-1 specifies the map to be loaded\n",
    "STAGE_NAME = 'SuperMarioBros-1-1-v0' # Standar versión\n",
    "#STAGE_NAME = 'SuperMarioBros-1-1-v3' # Rectangle versión\n",
    "env = gym_super_mario_bros.make(STAGE_NAME) #Create the enviroment"
   ],
   "metadata": {
    "id": "0v5U6jaCUs95",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:40.576913Z",
     "start_time": "2024-04-19T12:06:40.116038Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": ">The next step would be to specify the moves that our Mario could make. The enviroment brings us by default certain predefined movements, although we can create our own as we will see in the section of pre processing.",
   "metadata": {
    "id": "IGm80N_DYbQ3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "print(\"Simple Movements : \", SIMPLE_MOVEMENT)\n",
    "print(\"Complex Movements : \", COMPLEX_MOVEMENT)\n",
    "print(\"Right Only Movements : \", RIGHT_ONLY)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT) #specify the movements"
   ],
   "metadata": {
    "id": "ZhjKbt30ZdBe",
    "outputId": "a25d61af-64d0-444a-ec67-6bc931f84ac2",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:40.581917Z",
     "start_time": "2024-04-19T12:06:40.578152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'], ['A'], ['left']]\n",
      "Complex Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'], ['A'], ['left'], ['left', 'A'], ['left', 'B'], ['left', 'A', 'B'], ['down'], ['up']]\n",
      "Right Only Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B']]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": ">With these steps we can start playing with mario bros. ",
   "metadata": {
    "id": "m1zlnQFCaUPy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# done = True\n",
    "# for step in range(5):\n",
    "#     if done: # Done will be true if Mario dies in the game\n",
    "#         state = env.reset()\n",
    "#     state, reward, done, info = env.step(env.action_space.sample())\n",
    "#     env.render() # If we are running the program in Colab we will need to comment the rendering of the environment. \n",
    "# env.close()"
   ],
   "metadata": {
    "id": "_DEOBfXmax8J",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:40.756340Z",
     "start_time": "2024-04-19T12:06:40.582631Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## Pre-procesing",
   "metadata": {
    "id": "zMOYjQMFcK0N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "# Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, SubprocVecEnv, vec_monitor\n"
   ],
   "metadata": {
    "id": "U-vNJmTGcTAU",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:42.008082Z",
     "start_time": "2024-04-19T12:06:40.757983Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": ">This section analyzes the pre-processing that has been done to the environment. On the one hand, we have the SkipFrame function. By default, in each frame the game performs an action (a movement) and returns the reward for that action. What happens, is that to train the AI it is not necessary to make a move in each frame. That is why, the function executes the movement every X frames giving less work to do the training.",
   "metadata": {
    "id": "KhbkQuNweQN4"
   }
  },
  {
   "cell_type": "code",
   "source": "class SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        total_reward = 0.0\n        done = False\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, info",
   "metadata": {
    "id": "8cw3yGNOcc5K",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:42.011970Z",
     "start_time": "2024-04-19T12:06:42.008876Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": ">The second step is the re-scaling of our environment. By default the enviroment is given by 240*256 pixels. In order to optimize our model it is not necessary to have so many pixels and that is why we can rescale our enviroment to a smaller scale.",
   "metadata": {
    "id": "5HgxgmIhe6Ht"
   }
  },
  {
   "cell_type": "code",
   "source": "env = gym_super_mario_bros.make('SuperMarioBros-v0')\nenv = JoypadSpace(env, SIMPLE_MOVEMENT)\nstate = env.reset()\nprint(state.shape)",
   "metadata": {
    "id": "7_JRS2OueXBf",
    "outputId": "8f4e2cab-6276-476d-d186-912379eca9dc",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:42.184580Z",
     "start_time": "2024-04-19T12:06:42.012697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "class ResizeEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env, size):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        (oldh, oldw, oldc) = env.observation_space.shape\n",
    "        newshape = (size, size, oldc)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=newshape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        height, width, _ = self.observation_space.shape\n",
    "        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
    "        if frame.ndim == 2:\n",
    "            frame = frame[:,:,None]\n",
    "        return frame"
   ],
   "metadata": {
    "id": "d54mB36Ue8BA",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:42.188495Z",
     "start_time": "2024-04-19T12:06:42.185419Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
    "        self.current_score = 0\n",
    "        self.current_x = 0\n",
    "        self.current_x_count = 0\n",
    "        self.max_x = 0\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_score = 0\n",
    "        self.current_x = 0\n",
    "        self.current_x_count = 0\n",
    "        self.max_x = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        reward += max(0, info['x_pos'] - self.max_x)\n",
    "        if (info['x_pos'] - self.current_x) == 0:\n",
    "            self.current_x_count += 1\n",
    "        else:\n",
    "            self.current_x_count = 0\n",
    "        if info[\"flag_get\"]:\n",
    "            reward += 500\n",
    "            done = True\n",
    "            print(\"GOAL\")\n",
    "        if info[\"life\"] < 2:\n",
    "            reward -= 500\n",
    "            done = True\n",
    "        self.current_score = info[\"score\"]\n",
    "        self.max_x = max(self.max_x, self.current_x)\n",
    "        self.current_x = info[\"x_pos\"]\n",
    "        return state, reward / 10., done, info"
   ],
   "metadata": {
    "id": "GXK7ljdif9xG",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:42.192875Z",
     "start_time": "2024-04-19T12:06:42.189166Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": ">By default the environment is composed of the RGB room. This data is unnecessary when training our model and we will get better results if we convert our game to a grayscale.",
   "metadata": {
    "id": "pEeD3hH7h05q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "state = env.reset()\n",
    "print(\"RGB scale : \",state.shape)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "state = env.reset()\n",
    "print(\"Gray scale:\",state.shape)"
   ],
   "metadata": {
    "id": "DB40N-PwilSy",
    "outputId": "b2b26bff-d70c-4ef2-ed08-6fbb0588a5bd",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:42.367202Z",
     "start_time": "2024-04-19T12:06:42.193521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB scale :  (240, 256, 3)\n",
      "Gray scale: (240, 256, 1)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": ">Finally, it is important to group the frames when training. If you only train with one frame the AI will not be able to know where Mario or the enemies are moviing. This is why a FrameStack of 4 frames is created for training.",
   "metadata": {
    "id": "QLTHFpQXldnX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ],
   "metadata": {
    "id": "tPaZPDisleuy",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:42.373262Z",
     "start_time": "2024-04-19T12:06:42.370245Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": ">This is the final pre-processing",
   "metadata": {
    "id": "Yosni9AKlxe_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from typing import Callable\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "MOVEMENT = [[\"right\"], [\"right\", \"A\"]]\n",
    "STAGE_NAME = 'SuperMarioBros-1-1-v0'\n",
    "def make_mario_env(env_id: str, rank: int, seed: int = 0) -> Callable:\n",
    "    def _init() -> gym.Env:\n",
    "        env = gym_super_mario_bros.make(env_id)\n",
    "        env.reset(seed=seed + rank)\n",
    "        env = JoypadSpace(env, MOVEMENT)\n",
    "        #env = CustomRewardAndDoneEnv(env)\n",
    "        env = SkipFrame(env, skip=4)\n",
    "        env = GrayScaleObservation(env, keep_dim=True)\n",
    "        env = ResizeEnv(env, size=84)\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "def make_parallel_env(env_id, n_envs):\n",
    "    env = SubprocVecEnv([make_mario_env(env_id, i) for i in range(n_envs)])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    return env\n",
    "\n",
    "def make_single_env(env_id):\n",
    "    env = make_mario_env(env_id, 0)()\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    return env\n",
    "\n",
    "env = make_parallel_env(STAGE_NAME, 1)\n",
    "#env = make_single_env(STAGE_NAME)"
   ],
   "metadata": {
    "id": "HWcJ4YLkh1Xh",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:23:55.215064Z",
     "start_time": "2024-04-19T12:23:53.744686Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "# env.reset()\n",
    "# state, reward, done, info = env.step([0])\n",
    "# print('state:', state.shape) #Color scale, height, width, num of stacks"
   ],
   "metadata": {
    "id": "QFWwhsBml0Jm",
    "outputId": "5325afb2-9ba4-496f-a291-2f4764b56816",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.433875Z",
     "start_time": "2024-04-19T12:06:44.430006Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# def display_all_frame():\n",
    "#     plt.figure(figsize=(16,16))\n",
    "#     for idx in range(state.shape[3]):\n",
    "#         plt.subplot(1,4,idx+1)\n",
    "#         plt.imshow(state[0][:,:,idx])\n",
    "#     plt.show()"
   ],
   "metadata": {
    "id": "KL_uSrTVl-kf",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.438816Z",
     "start_time": "2024-04-19T12:06:44.435477Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "#display_all_frame()",
   "metadata": {
    "id": "95YVJ49xmAyW",
    "outputId": "789080b0-a34e-4ede-ad3a-eea3015962cd",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.444026Z",
     "start_time": "2024-04-19T12:06:44.440278Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "## Training of the model",
   "metadata": {
    "id": "FHPfiqJimJNn"
   }
  },
  {
   "cell_type": "code",
   "source": "# Import PPO for algos\nfrom stable_baselines3 import PPO\nimport torch as th\nfrom torch import nn\n\n# Import Base Callback for saving models\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor",
   "metadata": {
    "id": "_QPID_iJIAQK",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.449570Z",
     "start_time": "2024-04-19T12:06:44.445941Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# Model Param\n",
    "CHECK_FREQ_NUMB = 10_000\n",
    "TOTAL_TIMESTEP_NUMB = 1_000_000\n",
    "LEARNING_RATE = 0.0001\n",
    "GAE = 1.0\n",
    "ENT_COEF = 0.01\n",
    "N_STEPS = 512\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Test Param\n",
    "EPISODE_NUMBERS = 20\n",
    "MAX_TIMESTEP_TEST = 1000"
   ],
   "metadata": {
    "id": "fetFG9Y-KkVn",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.458949Z",
     "start_time": "2024-04-19T12:06:44.452215Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": ">Once the environment has been preprocessed, it is time to start training our AI model. In this case the stable-baseline3 PPO algorithm will be used due to its simplicity, but other alternatives such as DQN or DDQN can be explored. Before starting with the training, a convolutional neural network (CNN) has been created.",
   "metadata": {
    "id": "kSuCif-GitBJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": ">",
   "metadata": {
    "id": "ZWFOd0aYgz4L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MarioNet(BaseFeaturesExtractor):\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n",
    "        super(MarioNet, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")"
   ],
   "metadata": {
    "id": "eKYEsKiHKChE",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.469125Z",
     "start_time": "2024-04-19T12:06:44.460566Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": ">The next step consists of the creation of a file where the AI will save the results obtained in each iteration. In this way, later we will be able to visualize graphically the learning of our model.\n\n>In this case, the average score, the average starting time and the best score obtained will be saved for each iteration.",
   "metadata": {
    "id": "rznnR_7xizcQ"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.476875Z",
     "start_time": "2024-04-19T12:06:44.470580Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path('./model')\n",
    "save_dir.mkdir(parents=True)\n",
    "reward_log_path = (save_dir / 'reward_log.csv')"
   ],
   "metadata": {
    "id": "elCC-AerKSPD",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.482073Z",
     "start_time": "2024-04-19T12:06:44.478139Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "with open(reward_log_path, 'a') as f:\n",
    "    print('timesteps,reward,best_reward', file=f)"
   ],
   "metadata": {
    "id": "lAX834MzKXWG",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.488082Z",
     "start_time": "2024-04-19T12:06:44.484243Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": ">This callback function will be in charge of writing the aforementioned data to the file. This function will be executed automatically each time an iteration has been completed.",
   "metadata": {
    "id": "_wQPtcA5j1rr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        print('Start Training')\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.num_timesteps % self.check_freq == 0:\n",
    "            print('start test')\n",
    "            model_path = (save_dir / 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "            total_reward = [0] * EPISODE_NUMBERS\n",
    "            total_time = [0] * EPISODE_NUMBERS\n",
    "            best_reward = 0\n",
    "            test_env = make_single_env(STAGE_NAME)\n",
    "            for i in range(EPISODE_NUMBERS):\n",
    "              \n",
    "                state = test_env.reset()  # reset for each new trial\n",
    "                done = False\n",
    "                total_reward[i] = 0\n",
    "                total_time[i] = 0\n",
    "                while not done and total_time[i] < MAX_TIMESTEP_TEST:\n",
    "                    action, _ = model.predict(state)\n",
    "                    state, reward, done, info = test_env.step(action)\n",
    "                    total_reward[i] += reward[0]\n",
    "                    total_time[i] += 1\n",
    "\n",
    "                if total_reward[i] > best_reward:\n",
    "                    best_reward = total_reward[i]\n",
    "                    best_epoch = self.n_calls\n",
    "\n",
    "                state = test_env.reset()  # reset for each new trial\n",
    "\n",
    "            print('time steps:', self.num_timesteps, '/', TOTAL_TIMESTEP_NUMB)\n",
    "            print('average reward:', (sum(total_reward) / EPISODE_NUMBERS),\n",
    "                  'average time:', (sum(total_time) / EPISODE_NUMBERS),\n",
    "                  'best_reward:', best_reward)\n",
    "\n",
    "            with open(reward_log_path, 'a') as f:\n",
    "                print(self.n_calls, ',', sum(total_reward) / EPISODE_NUMBERS, ',', best_reward, file=f)\n",
    "\n",
    "        return True"
   ],
   "metadata": {
    "id": "ydS_MlH9KYwd",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:06:44.499959Z",
     "start_time": "2024-04-19T12:06:44.489809Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": ">Finally, all that remains is for our AI to start learning. ",
   "metadata": {
    "id": "Oxx7434mkYN2"
   }
  },
  {
   "cell_type": "code",
   "source": "callback = TrainAndLoggingCallback(check_freq=CHECK_FREQ_NUMB, save_path=save_dir)",
   "metadata": {
    "id": "5yXiD4toLChp",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:15:56.526131Z",
     "start_time": "2024-04-19T12:15:56.523491Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "model = PPO('CnnPolicy', env, verbose=0, policy_kwargs=policy_kwargs, tensorboard_log=save_dir, learning_rate=LEARNING_RATE, n_steps=N_STEPS,\n",
    "              batch_size=BATCH_SIZE, n_epochs=N_EPOCHS, gamma=GAMMA, gae_lambda=GAE, ent_coef=ENT_COEF)"
   ],
   "metadata": {
    "id": "qYFzPiWPLG5P",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:24:00.720750Z",
     "start_time": "2024-04-19T12:24:00.684574Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB)",
   "metadata": {
    "id": "1zwA7R2zLIy5",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:34:05.292568Z",
     "start_time": "2024-04-19T12:24:02.255259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/famoose/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 27, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTOTAL_TIMESTEP_NUMB\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:310\u001B[0m, in \u001B[0;36mPPO.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    299\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    307\u001B[0m     reset_num_timesteps: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    308\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPPO\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_log_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_log_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:267\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001B[0m\n\u001B[1;32m    264\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mrecord(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime/total_timesteps\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps, exclude\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    265\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdump(step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps)\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    269\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_end()\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:203\u001B[0m, in \u001B[0;36mPPO.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    200\u001B[0m actions \u001B[38;5;241m=\u001B[39m rollout_data\u001B[38;5;241m.\u001B[39mactions\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space, spaces\u001B[38;5;241m.\u001B[39mDiscrete):\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;66;03m# Convert discrete action from float to long\u001B[39;00m\n\u001B[0;32m--> 203\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[43mrollout_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    205\u001B[0m \u001B[38;5;66;03m# Re-sample the noise matrix because the log_std has changed\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_sde:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": "## Results and Conclusion",
   "metadata": {
    "id": "DF4Z0cWQkeHA"
   }
  },
  {
   "cell_type": "markdown",
   "source": ">This last section analyzes the results and conclusions of this project. As can be seen in the graphs, two different models have been trained, one using the standard set and the other using the rectangle set.\n\n>In the standard game, 1050000 iteractions have been executed, while in the rectangular game there have been 640000.  Although the rectangular model has been trained with much fewer iterations, the best model has similar results to the best standard model. \n\n>If we run the function that calculates the win rate we can see that both models have a 20% win rate.",
   "metadata": {
    "id": "VvWiZ6SFw3aY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "reward_log = pd.read_csv(\"reward_log_Standar.csv\", index_col='timesteps')\n",
    "reward_log.plot()"
   ],
   "metadata": {
    "id": "sbfGzJzfpzZ8",
    "outputId": "a689b59c-ebe0-4d67-a461-8c0a1528cc0c",
    "ExecuteTime": {
     "end_time": "2024-04-19T12:15:29.598131Z",
     "start_time": "2024-04-19T12:15:29.598060Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "reward_log = pd.read_csv(\"reward_log_Rectangle.csv\", index_col='timesteps')\n",
    "reward_log.plot()"
   ],
   "metadata": {
    "id": "QBWZxY3SqbSI",
    "outputId": "67a3f525-ec61-49cf-ff6b-8d9468b7ee54"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "reward_log = pd.read_csv(\"reward_log_Standar.csv\", index_col='timesteps')\nbest_epoch = reward_log['reward'].idxmax()\nprint('best epoch:', best_epoch)",
   "metadata": {
    "id": "GPm7fr1npz5F",
    "outputId": "17dff4d4-50a9-4d48-8502-6a25d14b658d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "best_model_path = os.path.join(save_dir, 'best_model_{}'.format(50000))\n",
    "model = PPO.load(best_model_path)"
   ],
   "metadata": {
    "id": "rzIoR_Ndp3AJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "env = make_single_env(STAGE_NAME)\n",
    "state = env.reset()\n",
    "done = False\n",
    "plays = 0\n",
    "wins = 0\n",
    "while plays < 100:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        state = env.reset() \n",
    "        if info[0][\"flag_get\"]:\n",
    "          wins += 1\n",
    "        plays += 1\n",
    "print(\"Model win rate: \" + str(wins) + \"%\")"
   ],
   "metadata": {
    "id": "ZoToaDDVp46c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "state = env.reset()\n",
    "plays = 0\n",
    "\n",
    "while plays < 100:\n",
    "    if done:\n",
    "        state = env.reset() \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render() #Only in local, not in Colab "
   ],
   "metadata": {
    "id": "U7dSVTueyhwo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": ">[Demo](https://youtube.com/shorts/jta7SegNNwM)\n<iframe width='560' height='315' src=\"https://youtube.com/shorts/jta7SegNNwM\"/>",
   "metadata": {
    "id": "mctsDSbNzQXH"
   }
  }
 ]
}
