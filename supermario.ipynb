{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 5152646,
     "sourceType": "datasetVersion",
     "datasetId": 2993870
    }
   ],
   "dockerImageVersionId": 30407,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# **Super Mario Bros. with Stable-Baseline3 PPO**\n\n",
   "metadata": {
    "id": "iezMhADyMDIi"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## First steps",
   "metadata": {
    "id": "iU7a5nPOUlK5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym_super_mario_bros\n",
    "#The 1-1 specifies the map to be loaded\n",
    "STAGE_NAME = 'SuperMarioBros-1-1-v0' # Standar versión\n",
    "#STAGE_NAME = 'SuperMarioBros-1-1-v3' # Rectangle versión\n",
    "env = gym_super_mario_bros.make(STAGE_NAME) #Create the enviroment"
   ],
   "metadata": {
    "id": "0v5U6jaCUs95",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:43.281601Z",
     "start_time": "2024-04-18T16:23:42.844572Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": ">The next step would be to specify the moves that our Mario could make. The enviroment brings us by default certain predefined movements, although we can create our own as we will see in the section of pre processing.",
   "metadata": {
    "id": "IGm80N_DYbQ3"
   }
  },
  {
   "cell_type": "code",
   "source": "from nes_py.wrappers import JoypadSpace\n\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nfrom gym_super_mario_bros.actions import COMPLEX_MOVEMENT\nfrom gym_super_mario_bros.actions import RIGHT_ONLY\nprint(\"Simple Movements : \", SIMPLE_MOVEMENT)\nprint(\"Complex Movements : \", COMPLEX_MOVEMENT)\nprint(\"Right Only Movements : \", RIGHT_ONLY)\nenv = JoypadSpace(env, SIMPLE_MOVEMENT) #specify the movements",
   "metadata": {
    "id": "ZhjKbt30ZdBe",
    "outputId": "a25d61af-64d0-444a-ec67-6bc931f84ac2",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:43.286525Z",
     "start_time": "2024-04-18T16:23:43.282731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'], ['A'], ['left']]\n",
      "Complex Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'], ['A'], ['left'], ['left', 'A'], ['left', 'B'], ['left', 'A', 'B'], ['down'], ['up']]\n",
      "Right Only Movements :  [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B']]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": ">With these steps we can start playing with mario bros. ",
   "metadata": {
    "id": "m1zlnQFCaUPy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# done = True\n",
    "# for step in range(5):\n",
    "#     if done: # Done will be true if Mario dies in the game\n",
    "#         state = env.reset()\n",
    "#     state, reward, done, info = env.step(env.action_space.sample())\n",
    "#     env.render() # If we are running the program in Colab we will need to comment the rendering of the environment. \n",
    "# env.close()"
   ],
   "metadata": {
    "id": "_DEOBfXmax8J",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:43.457916Z",
     "start_time": "2024-04-18T16:23:43.287230Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## Pre-procesing",
   "metadata": {
    "id": "zMOYjQMFcK0N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "# Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, SubprocVecEnv\n"
   ],
   "metadata": {
    "id": "U-vNJmTGcTAU",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:44.765685Z",
     "start_time": "2024-04-18T16:23:43.459381Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": ">This section analyzes the pre-processing that has been done to the environment. On the one hand, we have the SkipFrame function. By default, in each frame the game performs an action (a movement) and returns the reward for that action. What happens, is that to train the AI it is not necessary to make a move in each frame. That is why, the function executes the movement every X frames giving less work to do the training.",
   "metadata": {
    "id": "KhbkQuNweQN4"
   }
  },
  {
   "cell_type": "code",
   "source": "class SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        total_reward = 0.0\n        done = False\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, info",
   "metadata": {
    "id": "8cw3yGNOcc5K",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:44.769557Z",
     "start_time": "2024-04-18T16:23:44.766336Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": ">The second step is the re-scaling of our environment. By default the enviroment is given by 240*256 pixels. In order to optimize our model it is not necessary to have so many pixels and that is why we can rescale our enviroment to a smaller scale.",
   "metadata": {
    "id": "5HgxgmIhe6Ht"
   }
  },
  {
   "cell_type": "code",
   "source": "env = gym_super_mario_bros.make('SuperMarioBros-v0')\nenv = JoypadSpace(env, SIMPLE_MOVEMENT)\nstate = env.reset()\nprint(state.shape)",
   "metadata": {
    "id": "7_JRS2OueXBf",
    "outputId": "8f4e2cab-6276-476d-d186-912379eca9dc",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:44.941094Z",
     "start_time": "2024-04-18T16:23:44.770245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "class ResizeEnv(gym.ObservationWrapper):\n    def __init__(self, env, size):\n        gym.ObservationWrapper.__init__(self, env)\n        (oldh, oldw, oldc) = env.observation_space.shape\n        newshape = (size, size, oldc)\n        self.observation_space = gym.spaces.Box(low=0, high=255,\n            shape=newshape, dtype=np.uint8)\n\n    def observation(self, frame):\n        height, width, _ = self.observation_space.shape\n        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n        if frame.ndim == 2:\n            frame = frame[:,:,None]\n        return frame",
   "metadata": {
    "id": "d54mB36Ue8BA",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:44.944945Z",
     "start_time": "2024-04-18T16:23:44.941918Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": ">On the other hand, there is the environment reward function. By default,the reward function assumes the objective of the game is to move as far right as possible (increase the agent's x value), as fast as possible, without dying. To model this game, three separate variables compose the reward:\n\n\n\n1.   v: the difference in agent x values between states\n* in this case this is instantaneous velocity for the given step\n* v = x1 - x0\n* x0 is the x position before the step\n* x1 is the x position after the step\n* moving right ⇔ v > 0\n* moving left ⇔ v < 0\n* not moving ⇔ v = 0\n2. c: the difference in the game clock between frames\n* the penalty prevents the agent from standing still\n* c = c0 - c1\n* c0 is the clock reading before the step\n* c1 is the clock reading after the step\n* no clock tick ⇔ c = 0\n* clock tick ⇔ c < 0\n3. d: death penalty that penalizes the agent for dying in a state\n* this penalty encourages the agent to avoid death\n* alive ⇔ d = 0\n* dead ⇔ d = -15\n\n>r = v + c + d\n\n>The reward is clipped into the range (-15, 15).\nIt is in this function that we can start to try new things, such as creating an AI that prioritizes obtaining coins.",
   "metadata": {
    "id": "VC0Zo-taglOy"
   }
  },
  {
   "cell_type": "code",
   "source": "class CustomRewardAndDoneEnv(gym.Wrapper):\n    def __init__(self, env=None):\n        super(CustomRewardAndDoneEnv, self).__init__(env)\n        self.current_score = 0\n        self.current_x = 0\n        self.current_x_count = 0\n        self.max_x = 0\n    def reset(self, **kwargs):\n        self.current_score = 0\n        self.current_x = 0\n        self.current_x_count = 0\n        self.max_x = 0\n        return self.env.reset(**kwargs)\n    def step(self, action):\n        state, reward, done, info = self.env.step(action)\n        reward += max(0, info['x_pos'] - self.max_x)\n        if (info['x_pos'] - self.current_x) == 0:\n            self.current_x_count += 1\n        else:\n            self.current_x_count = 0\n        if info[\"flag_get\"]:\n            reward += 500\n            done = True\n            print(\"GOAL\")\n        if info[\"life\"] < 2:\n            reward -= 500\n            done = True\n        self.current_score = info[\"score\"]\n        self.max_x = max(self.max_x, self.current_x)\n        self.current_x = info[\"x_pos\"]\n        return state, reward / 10., done, info",
   "metadata": {
    "id": "GXK7ljdif9xG",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:44.949705Z",
     "start_time": "2024-04-18T16:23:44.945823Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": ">By default the environment is composed of the RGB room. This data is unnecessary when training our model and we will get better results if we convert our game to a grayscale.",
   "metadata": {
    "id": "pEeD3hH7h05q"
   }
  },
  {
   "cell_type": "code",
   "source": "env = gym_super_mario_bros.make('SuperMarioBros-v0')\nenv = JoypadSpace(env, SIMPLE_MOVEMENT)\nstate = env.reset()\nprint(\"RGB scale : \",state.shape)\nenv = GrayScaleObservation(env, keep_dim=True)\nstate = env.reset()\nprint(\"Gray scale:\",state.shape)",
   "metadata": {
    "id": "DB40N-PwilSy",
    "outputId": "b2b26bff-d70c-4ef2-ed08-6fbb0588a5bd",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:45.119907Z",
     "start_time": "2024-04-18T16:23:44.950431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB scale :  (240, 256, 3)\n",
      "Gray scale: (240, 256, 1)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": ">Finally, it is important to group the frames when training. If you only train with one frame the AI will not be able to know where Mario or the enemies are moviing. This is why a FrameStack of 4 frames is created for training.",
   "metadata": {
    "id": "QLTHFpQXldnX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ],
   "metadata": {
    "id": "tPaZPDisleuy",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:23:54.859208Z",
     "start_time": "2024-04-18T16:23:45.122491Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": ">This is the final pre-processing",
   "metadata": {
    "id": "Yosni9AKlxe_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MOVEMENT = [['left', 'A'], ['right', 'B'], ['right', 'A', 'B']]\n",
    "env = gym_super_mario_bros.make(STAGE_NAME)\n",
    "env = JoypadSpace(env, MOVEMENT)\n",
    "env = CustomRewardAndDoneEnv(env)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = ResizeEnv(env, size=84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "#env = SubprocVecEnv([lambda: env] * 8)\n",
    "# parallel environments\n",
    "\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ],
   "metadata": {
    "id": "HWcJ4YLkh1Xh",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:04.270779Z",
     "start_time": "2024-04-18T16:23:54.860112Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "env.reset()\nstate, reward, done, info = env.step([0])\nprint('state:', state.shape) #Color scale, height, width, num of stacks",
   "metadata": {
    "id": "QFWwhsBml0Jm",
    "outputId": "5325afb2-9ba4-496f-a291-2f4764b56816",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:04.504103Z",
     "start_time": "2024-04-18T16:24:04.271638Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::length_error: vector\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEOFError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m state, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep([\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate:\u001B[39m\u001B[38;5;124m'\u001B[39m, state\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;66;03m#Color scale, height, width, num of stacks\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:58\u001B[0m, in \u001B[0;36mVecFrameStack.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]]:\n\u001B[1;32m     55\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    Reset all environments\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype:disable=annotation-type-mismatch\u001B[39;00m\n\u001B[1;32m     60\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstackedobs\u001B[38;5;241m.\u001B[39mreset(observation)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:135\u001B[0m, in \u001B[0;36mSubprocVecEnv.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes:\n\u001B[1;32m    134\u001B[0m     remote\u001B[38;5;241m.\u001B[39msend((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m--> 135\u001B[0m obs \u001B[38;5;241m=\u001B[39m [remote\u001B[38;5;241m.\u001B[39mrecv() \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes]\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _flatten_obs(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space)\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:135\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes:\n\u001B[1;32m    134\u001B[0m     remote\u001B[38;5;241m.\u001B[39msend((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m--> 135\u001B[0m obs \u001B[38;5;241m=\u001B[39m [\u001B[43mremote\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes]\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _flatten_obs(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space)\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:255\u001B[0m, in \u001B[0;36m_ConnectionBase.recv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 255\u001B[0m buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_recv_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _ForkingPickler\u001B[38;5;241m.\u001B[39mloads(buf\u001B[38;5;241m.\u001B[39mgetbuffer())\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:419\u001B[0m, in \u001B[0;36mConnection._recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_recv_bytes\u001B[39m(\u001B[38;5;28mself\u001B[39m, maxsize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 419\u001B[0m     buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_recv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    420\u001B[0m     size, \u001B[38;5;241m=\u001B[39m struct\u001B[38;5;241m.\u001B[39munpack(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!i\u001B[39m\u001B[38;5;124m\"\u001B[39m, buf\u001B[38;5;241m.\u001B[39mgetvalue())\n\u001B[1;32m    421\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:388\u001B[0m, in \u001B[0;36mConnection._recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m remaining \u001B[38;5;241m==\u001B[39m size:\n\u001B[0;32m--> 388\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    390\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgot end of file during message\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mEOFError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "def display_all_frame():\n    plt.figure(figsize=(16,16))\n    for idx in range(state.shape[3]):\n        plt.subplot(1,4,idx+1)\n        plt.imshow(state[0][:,:,idx])\n    plt.show()",
   "metadata": {
    "id": "KL_uSrTVl-kf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "display_all_frame()",
   "metadata": {
    "id": "95YVJ49xmAyW",
    "outputId": "789080b0-a34e-4ede-ad3a-eea3015962cd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Training of the model",
   "metadata": {
    "id": "FHPfiqJimJNn"
   }
  },
  {
   "cell_type": "code",
   "source": "# Import PPO for algos\nfrom stable_baselines3 import PPO\nimport torch as th\nfrom torch import nn\n\n# Import Base Callback for saving models\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor",
   "metadata": {
    "id": "_QPID_iJIAQK",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:18.630249Z",
     "start_time": "2024-04-18T16:24:18.626328Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "# Model Param\nCHECK_FREQ_NUMB = 10000\nTOTAL_TIMESTEP_NUMB = 5000000\nLEARNING_RATE = 0.0001\nGAE = 1.0\nENT_COEF = 0.01\nN_STEPS = 512\nGAMMA = 0.9\nBATCH_SIZE = 64\nN_EPOCHS = 10\n\n# Test Param\nEPISODE_NUMBERS = 20\nMAX_TIMESTEP_TEST = 1000",
   "metadata": {
    "id": "fetFG9Y-KkVn",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:21.272313Z",
     "start_time": "2024-04-18T16:24:21.269847Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": ">Once the environment has been preprocessed, it is time to start training our AI model. In this case the stable-baseline3 PPO algorithm will be used due to its simplicity, but other alternatives such as DQN or DDQN can be explored. Before starting with the training, a convolutional neural network (CNN) has been created.",
   "metadata": {
    "id": "kSuCif-GitBJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": ">",
   "metadata": {
    "id": "ZWFOd0aYgz4L"
   }
  },
  {
   "cell_type": "code",
   "source": "class MarioNet(BaseFeaturesExtractor):\n\n    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n        super(MarioNet, self).__init__(observation_space, features_dim)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=MarioNet,\n    features_extractor_kwargs=dict(features_dim=512),\n)",
   "metadata": {
    "id": "eKYEsKiHKChE",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:23.895834Z",
     "start_time": "2024-04-18T16:24:23.889880Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": ">The next step consists of the creation of a file where the AI will save the results obtained in each iteration. In this way, later we will be able to visualize graphically the learning of our model.\n\n>In this case, the average score, the average starting time and the best score obtained will be saved for each iteration.",
   "metadata": {
    "id": "rznnR_7xizcQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "# if model exist remove\n",
    "if os.path.exists('model'):\n",
    "    shutil.rmtree('model')\n",
    "\n",
    "save_dir = Path('./model')\n",
    "save_dir.mkdir(parents=True)\n",
    "reward_log_path = (save_dir / 'reward_log.csv')"
   ],
   "metadata": {
    "id": "elCC-AerKSPD",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:34.531905Z",
     "start_time": "2024-04-18T16:24:34.526890Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "with open(reward_log_path, 'a') as f:\n",
    "    print('timesteps,reward,best_reward', file=f)"
   ],
   "metadata": {
    "id": "lAX834MzKXWG",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:37.864445Z",
     "start_time": "2024-04-18T16:24:37.861016Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": ">This callback function will be in charge of writing the aforementioned data to the file. This function will be executed automatically each time an iteration has been completed.",
   "metadata": {
    "id": "_wQPtcA5j1rr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = (save_dir / 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "            total_reward = [0] * EPISODE_NUMBERS\n",
    "            total_time = [0] * EPISODE_NUMBERS\n",
    "            best_reward = 0\n",
    "\n",
    "            for i in range(EPISODE_NUMBERS):\n",
    "                state = env.reset()  # reset for each new trial\n",
    "                done = False\n",
    "                total_reward[i] = 0\n",
    "                total_time[i] = 0\n",
    "                while not done and total_time[i] < MAX_TIMESTEP_TEST:\n",
    "                    action, _ = model.predict(state)\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    total_reward[i] += reward[0]\n",
    "                    total_time[i] += 1\n",
    "\n",
    "                if total_reward[i] > best_reward:\n",
    "                    best_reward = total_reward[i]\n",
    "                    best_epoch = self.n_calls\n",
    "\n",
    "                state = env.reset()  # reset for each new trial\n",
    "\n",
    "            print('time steps:', self.n_calls, '/', TOTAL_TIMESTEP_NUMB)\n",
    "            print('average reward:', (sum(total_reward) / EPISODE_NUMBERS),\n",
    "                  'average time:', (sum(total_time) / EPISODE_NUMBERS),\n",
    "                  'best_reward:', best_reward)\n",
    "\n",
    "            with open(reward_log_path, 'a') as f:\n",
    "                print(self.n_calls, ',', sum(total_reward) / EPISODE_NUMBERS, ',', best_reward, file=f)\n",
    "\n",
    "        return True"
   ],
   "metadata": {
    "id": "ydS_MlH9KYwd",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:40.261755Z",
     "start_time": "2024-04-18T16:24:40.254862Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": ">Finally, all that remains is for our AI to start learning. ",
   "metadata": {
    "id": "Oxx7434mkYN2"
   }
  },
  {
   "cell_type": "code",
   "source": "callback = TrainAndLoggingCallback(check_freq=CHECK_FREQ_NUMB, save_path=save_dir)",
   "metadata": {
    "id": "5yXiD4toLChp",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:46.809025Z",
     "start_time": "2024-04-18T16:24:46.805934Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "model = PPO('CnnPolicy', env, verbose=0, policy_kwargs=policy_kwargs, tensorboard_log=save_dir, learning_rate=LEARNING_RATE, n_steps=N_STEPS,\n              batch_size=BATCH_SIZE, n_epochs=N_EPOCHS, gamma=GAMMA, gae_lambda=GAE, ent_coef=ENT_COEF)",
   "metadata": {
    "id": "qYFzPiWPLG5P",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:49.415714Z",
     "start_time": "2024-04-18T16:24:48.950138Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, callback=callback)",
   "metadata": {
    "id": "1zwA7R2zLIy5",
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:50.799337Z",
     "start_time": "2024-04-18T16:24:50.662591Z"
    }
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBrokenPipeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTOTAL_TIMESTEP_NUMB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:310\u001B[0m, in \u001B[0;36mPPO.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    299\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    307\u001B[0m     reset_num_timesteps: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    308\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPPO\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_log_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_log_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:239\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    227\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    235\u001B[0m     reset_num_timesteps: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnPolicyAlgorithm\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    237\u001B[0m     iteration \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 239\u001B[0m     total_timesteps, callback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_learn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_log_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_log_name\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    243\u001B[0m     callback\u001B[38;5;241m.\u001B[39mon_training_start(\u001B[38;5;28mlocals\u001B[39m(), \u001B[38;5;28mglobals\u001B[39m())\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:446\u001B[0m, in \u001B[0;36mBaseAlgorithm._setup_learn\u001B[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name)\u001B[0m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;66;03m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001B[39;00m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reset_num_timesteps \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype: disable=annotation-type-mismatch\u001B[39;00m\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_episode_starts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mnum_envs,), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m)\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:110\u001B[0m, in \u001B[0;36mVecTransposeImage.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict]:\n\u001B[1;32m    107\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;124;03m    Reset all environments\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_observations(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:58\u001B[0m, in \u001B[0;36mVecFrameStack.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]]:\n\u001B[1;32m     55\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    Reset all environments\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype:disable=annotation-type-mismatch\u001B[39;00m\n\u001B[1;32m     60\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstackedobs\u001B[38;5;241m.\u001B[39mreset(observation)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation\n",
      "File \u001B[0;32m~/PycharmProjects/bsc_thesis/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:134\u001B[0m, in \u001B[0;36mSubprocVecEnv.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvObs:\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes:\n\u001B[0;32m--> 134\u001B[0m         \u001B[43mremote\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m     obs \u001B[38;5;241m=\u001B[39m [remote\u001B[38;5;241m.\u001B[39mrecv() \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes]\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _flatten_obs(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space)\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:211\u001B[0m, in \u001B[0;36m_ConnectionBase.send\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_writable()\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_ForkingPickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:416\u001B[0m, in \u001B[0;36mConnection._send_bytes\u001B[0;34m(self, buf)\u001B[0m\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send(buf)\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    412\u001B[0m     \u001B[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001B[39;00m\n\u001B[1;32m    413\u001B[0m     \u001B[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001B[39;00m\n\u001B[1;32m    414\u001B[0m     \u001B[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001B[39;00m\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001B[39;00m\n\u001B[0;32m--> 416\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send\u001B[49m\u001B[43m(\u001B[49m\u001B[43mheader\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:373\u001B[0m, in \u001B[0;36mConnection._send\u001B[0;34m(self, buf, write)\u001B[0m\n\u001B[1;32m    371\u001B[0m remaining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(buf)\n\u001B[1;32m    372\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 373\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    374\u001B[0m     remaining \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m n\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m remaining \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mBrokenPipeError\u001B[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "## Results and Conclusion",
   "metadata": {
    "id": "DF4Z0cWQkeHA"
   }
  },
  {
   "cell_type": "markdown",
   "source": ">This last section analyzes the results and conclusions of this project. As can be seen in the graphs, two different models have been trained, one using the standard set and the other using the rectangle set.\n\n>In the standard game, 1050000 iteractions have been executed, while in the rectangular game there have been 640000.  Although the rectangular model has been trained with much fewer iterations, the best model has similar results to the best standard model. \n\n>If we run the function that calculates the win rate we can see that both models have a 20% win rate.",
   "metadata": {
    "id": "VvWiZ6SFw3aY"
   }
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nreward_log = pd.read_csv(\"reward_log_Standar.csv\", index_col='timesteps')\nreward_log.plot()",
   "metadata": {
    "id": "sbfGzJzfpzZ8",
    "outputId": "a689b59c-ebe0-4d67-a461-8c0a1528cc0c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nreward_log = pd.read_csv(\"reward_log_Rectangle.csv\", index_col='timesteps')\nreward_log.plot()",
   "metadata": {
    "id": "QBWZxY3SqbSI",
    "outputId": "67a3f525-ec61-49cf-ff6b-8d9468b7ee54"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "reward_log = pd.read_csv(\"reward_log_Standar.csv\", index_col='timesteps')\nbest_epoch = reward_log['reward'].idxmax()\nprint('best epoch:', best_epoch)",
   "metadata": {
    "id": "GPm7fr1npz5F",
    "outputId": "17dff4d4-50a9-4d48-8502-6a25d14b658d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "best_model_path = os.path.join(save_dir, 'best_model_{}'.format(best_epoch))\nmodel = PPO.load(best_model_path)",
   "metadata": {
    "id": "rzIoR_Ndp3AJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "state = env.reset()\ndone = True\nplays = 0;\nwins = 0;\nwhile plays < 100:\n    if done:\n        state = env.reset() \n        if info[0][\"flag_get\"]:\n          wins += 1\n        plays += 1\n    action, _ = model.predict(state)\n    state, reward, done, info = env.step(action)\nprint(\"Model win rate: \" + str(wins) + \"%\")",
   "metadata": {
    "id": "ZoToaDDVp46c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "state = env.reset()\n\nwhile plays < 100:\n    if done:\n        state = env.reset() \n    action, _ = model.predict(state)\n    state, reward, done, info = env.step(action)\n    #env.render() #Only in local, not in Colab ",
   "metadata": {
    "id": "U7dSVTueyhwo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": ">[Demo](https://youtube.com/shorts/jta7SegNNwM)\n<iframe width='560' height='315' src=\"https://youtube.com/shorts/jta7SegNNwM\"/>",
   "metadata": {
    "id": "mctsDSbNzQXH"
   }
  }
 ]
}
